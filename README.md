#GPT-using-pytorch

A minimal implementation of a GPT-like transformer model trained from scratch using PyTorch. This is a proof-of-concept model with 2 layers,
trained on a small dataset for educational purposes. Ideal for learning how transformers work under the hood!

## ‚ú® Features  
- **From-scratch implementation** of transformer architecture (self-attention, positional encoding, etc.).  
- Minimal and clean PyTorch code for easy understanding.  
- Example training and inference scripts included.

## üõ†Ô∏è Installation  
1. Clone the repository:  
   git clone https://github.com/Rohit2sali/GPT-using-pytorch.git
   After cloning the repository, run the train.py file. All the hyperparameters are there in it, you can change as you want.

## üôè Acknowledgments  
- Inspired by [Karpathy‚Äôs nanoGPT](https://github.com/karpathy/nanoGPT).  
- Dataset: [TinyShakespeare](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt).  
