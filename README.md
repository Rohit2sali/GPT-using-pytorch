# TokenPredictionUsingDecoder

A minimal implementation of a decoder only transformer model trained from scratch using PyTorch. This is a proof-of-concept model with 2 layers,
trained on a small dataset for educational purposes. Ideal for learning how transformers work under the hood!

## ‚ú® Features  
- Implementation of transformers decoder only architecture (self-attention, positional encoding, etc.).  
- Minimal and clean PyTorch code for easy understanding.  
- Example training and inference scripts included.

## üõ†Ô∏è Installation  
1. Clone the repository:  
   git clone https://github.com/Rohit2sali/TokenPredictionUsingDecoder.git
   After cloning the repository, run the train.py file. All the hyperparameters are there in it, you can change as you want.

## üôè Acknowledgments  
- Inspired by [Karpathy‚Äôs nanoGPT](https://github.com/karpathy/nanoGPT).  
- Dataset: [TinyShakespeare](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt).  
